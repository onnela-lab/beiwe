{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef35a192",
   "metadata": {},
   "source": [
    "# Download the data summary table (same as \"Click here to download a csv of the current summary statistics for your study.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "68e4baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "#Use this cell if you've moved this notebook somewhere else\n",
    "#sys.path.insert(0, \"/path/to/repo/beiwe/code\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "887cae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from matplotlib) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from matplotlib) (3.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ForestEnv/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8c668541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "872d1514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using API URL: https://studies.beiwe.org/\n",
      "Access key loaded: GQVW5/ybRd...\n"
     ]
    }
   ],
   "source": [
    "# Load credentials from keyring file\n",
    "import data_summaries\n",
    "kr = data_summaries.read_keyring(\"/Users/zhusiyao/Documents/Beiwe Projects/data_volume_summaries/keyring_studies.py\")\n",
    "\n",
    "# Extract credentials from keyring\n",
    "ACCESS_KEY = os.environ.get('BEIWE_ACCESS_KEY')\n",
    "SECRET_KEY = os.environ.get('BEIWE_SECRET_KEY')\n",
    "API_URL_BASE = os.environ.get('BEIWE_URL', 'https://studies.beiwe.org/')\n",
    "\n",
    "print(f\"Using API URL: {API_URL_BASE}\")\n",
    "print(f\"Access key loaded: {ACCESS_KEY[:10]}...\" if ACCESS_KEY else \"No access key found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b80b598",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summaries_file_path = \"data_volume.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a19be54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to download and filter participant data\n",
    "import json\n",
    "\n",
    "def make_session(total_retries=3, backoff=0.5):\n",
    "    \"\"\"Create a requests session with retry logic\"\"\"\n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=total_retries,\n",
    "        connect=total_retries,\n",
    "        read=total_retries,\n",
    "        backoff_factor=backoff,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"POST\"]),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    return s\n",
    "\n",
    "def get_participant_table_data(study_id, \n",
    "                               data_format=\"csv\",   # \"csv\", \"json\", \"json_table\"\n",
    "                               out_path=None,\n",
    "                               timeout=120):\n",
    "    \"\"\"\n",
    "    Calls /get-participant-table-data/v1 and returns or saves the result.\n",
    "    \"\"\"\n",
    "    if not API_URL_BASE.endswith(\"/\"):\n",
    "        base = API_URL_BASE + \"/\"\n",
    "    else:\n",
    "        base = API_URL_BASE\n",
    "    url = base + \"get-participant-table-data/v1\"\n",
    "\n",
    "    payload = {\n",
    "        \"access_key\": ACCESS_KEY,\n",
    "        \"secret_key\": SECRET_KEY,\n",
    "        \"study_id\": study_id,\n",
    "        \"data_format\": data_format,\n",
    "    }\n",
    "\n",
    "    session = make_session()\n",
    "    resp = session.post(url, data=payload, timeout=timeout, allow_redirects=False)\n",
    "\n",
    "    if resp.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {resp.status_code}: {resp.text[:300]}\")\n",
    "\n",
    "    # Handle CSV\n",
    "    if data_format == \"csv\":\n",
    "        out_path = out_path or \"participant_table.csv\"\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            f.write(resp.content)\n",
    "        print(\"Saved CSV to:\", os.path.abspath(out_path))\n",
    "        return out_path\n",
    "\n",
    "    # Handle JSON\n",
    "    obj = resp.json()\n",
    "    if out_path:\n",
    "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "        print(\"Saved JSON to:\", os.path.abspath(out_path))\n",
    "        return out_path\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "def get_active_participants(study_id):\n",
    "    \"\"\"\n",
    "    Download participant data and filter out 'Not Registered' and 'Permanently Retired' participants.\n",
    "    Returns a list of active participant IDs.\n",
    "    \"\"\"\n",
    "    print(\"Downloading participant data...\")\n",
    "    \n",
    "    # Get participant data as JSON\n",
    "    participants_data = get_participant_table_data(study_id, data_format=\"json\")\n",
    "    \n",
    "    # Filter out inactive participants\n",
    "    active_participants = []\n",
    "    excluded_count = 0\n",
    "    \n",
    "    for participant in participants_data:\n",
    "        status = participant.get('Status', '')\n",
    "        participant_id = participant.get('Patient ID', '')\n",
    "        \n",
    "        if status in ['Not Registered', 'Permanently Retired']:\n",
    "            excluded_count += 1\n",
    "            print(f\"Excluding participant {participant_id} with status: {status}\")\n",
    "        else:\n",
    "            active_participants.append(participant_id)\n",
    "    \n",
    "    print(f\"\\nFiltered out {excluded_count} participants with 'Not Registered' or 'Permanently Retired' status\")\n",
    "    print(f\"Active participants remaining: {len(active_participants)}\")\n",
    "    \n",
    "    return active_participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94a89305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading participant data...\n",
      "Excluding participant 1anurea6 with status: Permanently Retired\n",
      "Excluding participant 1bllhfi7 with status: Permanently Retired\n",
      "Excluding participant 1f8ujz41 with status: Permanently Retired\n",
      "Excluding participant 1ib9r56g with status: Not Registered\n",
      "Excluding participant 1sshhk6u with status: Permanently Retired\n",
      "Excluding participant 29dam7cc with status: Permanently Retired\n",
      "Excluding participant 33yib9v1 with status: Permanently Retired\n",
      "Excluding participant 3hpam6bh with status: Not Registered\n",
      "Excluding participant 3syylhuo with status: Permanently Retired\n",
      "Excluding participant 3vsmm961 with status: Permanently Retired\n",
      "Excluding participant 3xwxyr8p with status: Permanently Retired\n",
      "Excluding participant 44ggnee7 with status: Permanently Retired\n",
      "Excluding participant 4a517z7e with status: Not Registered\n",
      "Excluding participant 4ahuk17u with status: Permanently Retired\n",
      "Excluding participant 4ggpbwfj with status: Permanently Retired\n",
      "Excluding participant 4jdph3hy with status: Permanently Retired\n",
      "Excluding participant 4lkyq346 with status: Permanently Retired\n",
      "Excluding participant 4qg4ghee with status: Permanently Retired\n",
      "Excluding participant 4sl276dm with status: Permanently Retired\n",
      "Excluding participant 5rsoj9bd with status: Permanently Retired\n",
      "Excluding participant 62ege46u with status: Permanently Retired\n",
      "Excluding participant 6471v7gr with status: Permanently Retired\n",
      "Excluding participant 6bgfd8eo with status: Permanently Retired\n",
      "Excluding participant 6cee5lai with status: Permanently Retired\n",
      "Excluding participant 6d2sakpx with status: Permanently Retired\n",
      "Excluding participant 6dkw2zd9 with status: Permanently Retired\n",
      "Excluding participant 6j1lnpb6 with status: Permanently Retired\n",
      "Excluding participant 6uqywzdm with status: Permanently Retired\n",
      "Excluding participant 6yq7anur with status: Permanently Retired\n",
      "Excluding participant 71j3xz7m with status: Not Registered\n",
      "Excluding participant 7etwlarw with status: Permanently Retired\n",
      "Excluding participant 81jervii with status: Permanently Retired\n",
      "Excluding participant 89v1avmz with status: Permanently Retired\n",
      "Excluding participant 8bqvlii4 with status: Permanently Retired\n",
      "Excluding participant 8hszphf9 with status: Permanently Retired\n",
      "Excluding participant 8rx3kyrz with status: Permanently Retired\n",
      "Excluding participant 8wcv79ly with status: Permanently Retired\n",
      "Excluding participant 9l8bep2f with status: Not Registered\n",
      "Excluding participant 9pg1ror7 with status: Permanently Retired\n",
      "Excluding participant 9u5qzay4 with status: Permanently Retired\n",
      "Excluding participant a2wiazls with status: Permanently Retired\n",
      "Excluding participant a8ai8l3h with status: Permanently Retired\n",
      "Excluding participant acxdh2cq with status: Permanently Retired\n",
      "Excluding participant adwnelex with status: Permanently Retired\n",
      "Excluding participant ag5wdmi3 with status: Permanently Retired\n",
      "Excluding participant aqpn1oh4 with status: Permanently Retired\n",
      "Excluding participant be3smgzk with status: Permanently Retired\n",
      "Excluding participant boumgwee with status: Permanently Retired\n",
      "Excluding participant cbh8yze7 with status: Permanently Retired\n",
      "Excluding participant chytqvef with status: Permanently Retired\n",
      "Excluding participant ck2f8uml with status: Permanently Retired\n",
      "Excluding participant cshvodgb with status: Permanently Retired\n",
      "Excluding participant ct2nr49d with status: Permanently Retired\n",
      "Excluding participant ctp8953m with status: Permanently Retired\n",
      "Excluding participant cvfd3sil with status: Permanently Retired\n",
      "Excluding participant d188d2ib with status: Permanently Retired\n",
      "Excluding participant d83ciij6 with status: Not Registered\n",
      "Excluding participant d94vtyxq with status: Not Registered\n",
      "Excluding participant dlio244z with status: Permanently Retired\n",
      "Excluding participant e15hkfui with status: Permanently Retired\n",
      "Excluding participant e8wz2rdf with status: Permanently Retired\n",
      "Excluding participant eaf7472o with status: Permanently Retired\n",
      "Excluding participant ehmrvrsf with status: Permanently Retired\n",
      "Excluding participant elmsldp5 with status: Permanently Retired\n",
      "Excluding participant elspw6qp with status: Permanently Retired\n",
      "Excluding participant enylyud4 with status: Permanently Retired\n",
      "Excluding participant eq3srjdy with status: Permanently Retired\n",
      "Excluding participant euksje2e with status: Permanently Retired\n",
      "Excluding participant fhjswy1u with status: Permanently Retired\n",
      "Excluding participant frf7ayr5 with status: Permanently Retired\n",
      "Excluding participant fvrrniok with status: Permanently Retired\n",
      "Excluding participant genca1fz with status: Permanently Retired\n",
      "Excluding participant gmpgx2yd with status: Permanently Retired\n",
      "Excluding participant gq4s82qs with status: Permanently Retired\n",
      "Excluding participant h323v5fd with status: Permanently Retired\n",
      "Excluding participant h3pocaum with status: Permanently Retired\n",
      "Excluding participant hoqafprw with status: Permanently Retired\n",
      "Excluding participant hure3dcb with status: Permanently Retired\n",
      "Excluding participant iealausr with status: Permanently Retired\n",
      "Excluding participant iibftjbj with status: Permanently Retired\n",
      "Excluding participant ity51q6h with status: Permanently Retired\n",
      "Excluding participant iwqpcimr with status: Permanently Retired\n",
      "Excluding participant j96jck8o with status: Permanently Retired\n",
      "Excluding participant jgun8uls with status: Permanently Retired\n",
      "Excluding participant ji6kggnh with status: Permanently Retired\n",
      "Excluding participant js9dzj4m with status: Permanently Retired\n",
      "Excluding participant juqyd3vu with status: Permanently Retired\n",
      "Excluding participant k33n5mag with status: Permanently Retired\n",
      "Excluding participant k98ncfwh with status: Permanently Retired\n",
      "Excluding participant kb5mxovq with status: Permanently Retired\n",
      "Excluding participant kc17pj4t with status: Permanently Retired\n",
      "Excluding participant km698xy3 with status: Permanently Retired\n",
      "Excluding participant l2e3gzih with status: Permanently Retired\n",
      "Excluding participant l2jj4m6b with status: Permanently Retired\n",
      "Excluding participant lr8btjzf with status: Not Registered\n",
      "Excluding participant luy1yvhc with status: Permanently Retired\n",
      "Excluding participant lwfhx536 with status: Permanently Retired\n",
      "Excluding participant ly7qnqkp with status: Permanently Retired\n",
      "Excluding participant m58fpko8 with status: Permanently Retired\n",
      "Excluding participant m9x9l5rj with status: Permanently Retired\n",
      "Excluding participant mhwnmw12 with status: Permanently Retired\n",
      "Excluding participant mjumw9e3 with status: Permanently Retired\n",
      "Excluding participant mn4y1zpr with status: Permanently Retired\n",
      "Excluding participant mvohh4ld with status: Not Registered\n",
      "Excluding participant ng7mkyit with status: Not Registered\n",
      "Excluding participant nm6ahjsz with status: Permanently Retired\n",
      "Excluding participant nsm5mpxt with status: Permanently Retired\n",
      "Excluding participant nstx4ald with status: Permanently Retired\n",
      "Excluding participant nztotsgy with status: Permanently Retired\n",
      "Excluding participant o72ugjhg with status: Permanently Retired\n",
      "Excluding participant od8ma2vg with status: Not Registered\n",
      "Excluding participant omha7xt8 with status: Permanently Retired\n",
      "Excluding participant ooqyppkj with status: Permanently Retired\n",
      "Excluding participant owm7ytjt with status: Permanently Retired\n",
      "Excluding participant oz7nz5qd with status: Permanently Retired\n",
      "Excluding participant peb38bk5 with status: Permanently Retired\n",
      "Excluding participant pewlnfhy with status: Permanently Retired\n",
      "Excluding participant q1m8cls3 with status: Permanently Retired\n",
      "Excluding participant q6a67gi2 with status: Permanently Retired\n",
      "Excluding participant qdtulokz with status: Permanently Retired\n",
      "Excluding participant qxgs5tdn with status: Permanently Retired\n",
      "Excluding participant r8y71qnz with status: Permanently Retired\n",
      "Excluding participant ra5aq9d5 with status: Permanently Retired\n",
      "Excluding participant rafuzb58 with status: Permanently Retired\n",
      "Excluding participant rakffpyz with status: Permanently Retired\n",
      "Excluding participant rcmc2dws with status: Not Registered\n",
      "Excluding participant reo62ltw with status: Permanently Retired\n",
      "Excluding participant ri65gcde with status: Permanently Retired\n",
      "Excluding participant rjdggjew with status: Permanently Retired\n",
      "Excluding participant rjl4z12e with status: Not Registered\n",
      "Excluding participant rjtew49f with status: Permanently Retired\n",
      "Excluding participant rprigneb with status: Permanently Retired\n",
      "Excluding participant rstokk1a with status: Permanently Retired\n",
      "Excluding participant rz1t9mpt with status: Permanently Retired\n",
      "Excluding participant sfctppup with status: Permanently Retired\n",
      "Excluding participant sfih1cjt with status: Permanently Retired\n",
      "Excluding participant skv7wklf with status: Not Registered\n",
      "Excluding participant slgm2rxi with status: Permanently Retired\n",
      "Excluding participant srntczw7 with status: Permanently Retired\n",
      "Excluding participant suo8czlz with status: Permanently Retired\n",
      "Excluding participant swc8funz with status: Permanently Retired\n",
      "Excluding participant t9fev5hz with status: Permanently Retired\n",
      "Excluding participant tbagwbl9 with status: Permanently Retired\n",
      "Excluding participant ter7pxoe with status: Permanently Retired\n",
      "Excluding participant tkwn9148 with status: Permanently Retired\n",
      "Excluding participant tl57unkl with status: Permanently Retired\n",
      "Excluding participant tpr4b4hj with status: Permanently Retired\n",
      "Excluding participant ttcznvyi with status: Permanently Retired\n",
      "Excluding participant u4s4jruu with status: Permanently Retired\n",
      "Excluding participant ubeks4fp with status: Permanently Retired\n",
      "Excluding participant uewaotso with status: Permanently Retired\n",
      "Excluding participant uiz2mt3n with status: Permanently Retired\n",
      "Excluding participant vis6vd5c with status: Permanently Retired\n",
      "Excluding participant wglu9o68 with status: Permanently Retired\n",
      "Excluding participant wihq7mla with status: Permanently Retired\n",
      "Excluding participant wit5qifi with status: Permanently Retired\n",
      "Excluding participant wxokpxg8 with status: Permanently Retired\n",
      "Excluding participant x5n8ymny with status: Permanently Retired\n",
      "Excluding participant xigutsxt with status: Permanently Retired\n",
      "Excluding participant yd8makxv with status: Permanently Retired\n",
      "Excluding participant yfgkdoff with status: Permanently Retired\n",
      "Excluding participant yo6y8jnx with status: Permanently Retired\n",
      "Excluding participant yp41kwzi with status: Permanently Retired\n",
      "Excluding participant zmqqnw91 with status: Permanently Retired\n",
      "Excluding participant zqunj9oe with status: Not Registered\n",
      "\n",
      "Filtered out 165 participants with 'Not Registered' or 'Permanently Retired' status\n",
      "Active participants remaining: 182\n"
     ]
    }
   ],
   "source": [
    "# Get active participants (filtered)\n",
    "study_id = \"m4z54N5SU7Eqq2LbwmxQd2UN\"\n",
    "active_participants = get_active_participants(study_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0528b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "study_id =  \"m4z54N5SU7Eqq2LbwmxQd2UN\"\n",
    "data_summaries.get_data_summaries(study_id,\n",
    "        output_file_path = data_summaries_file_path,\n",
    "        keyring = kr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457298c9",
   "metadata": {},
   "source": [
    "# Downstream Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d9ef07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please set lookback window\n",
    "x = 7  # lookback window in days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0b85ae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total participants in data: 341\n",
      "Active participants after filtering: 182\n",
      "Saved day-level collection matrix for last 7 days to: daily_sensor_collection_last_7_days.csv\n",
      "Columns: participant_id, date, beiwe_accelerometer_bytes, beiwe_gps_bytes, has_accel, has_gps, has_both, missing_any\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ========= user parameters =========\n",
    "CSV_PATH = \"data_volume.csv\"\n",
    "tz = \"America/New_York\"\n",
    "\n",
    "# ========= load =========\n",
    "csv_path = Path(CSV_PATH)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "required = {\"date\", \"study_id\", \"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\", \"participant_id\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV missing required columns: {sorted(missing)}\")\n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"date\"])\n",
    "df[\"beiwe_accelerometer_bytes\"] = pd.to_numeric(df[\"beiwe_accelerometer_bytes\"], errors=\"coerce\").fillna(0)\n",
    "df[\"beiwe_gps_bytes\"] = pd.to_numeric(df[\"beiwe_gps_bytes\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "if study_id is not None:\n",
    "    df = df[df[\"study_id\"] == study_id]\n",
    "\n",
    "# ========= FILTER OUT INACTIVE PARTICIPANTS =========\n",
    "print(f\"Total participants in data: {df['participant_id'].nunique()}\")\n",
    "df = df[df[\"participant_id\"].isin(active_participants)]\n",
    "print(f\"Active participants after filtering: {df['participant_id'].nunique()}\")\n",
    "\n",
    "# ========= time window =========\n",
    "# Exclude today - look back from yesterday and back x days\n",
    "yesterday = pd.Timestamp.now(tz=tz).normalize() - pd.Timedelta(days=1)\n",
    "start = yesterday - pd.Timedelta(days=x - 1)  # inclusive [start, yesterday]\n",
    "\n",
    "# normalize to local calendar dates (naive) for day-level logic\n",
    "# Handle both naive and timezone-aware datetimes\n",
    "if df[\"date\"].dt.tz is not None:\n",
    "    # If timezone-aware, convert to naive and normalize\n",
    "    df[\"local_date\"] = df[\"date\"].dt.tz_localize(None).dt.normalize()\n",
    "else:\n",
    "    # If already naive, just normalize (sets time to 00:00:00)\n",
    "    df[\"local_date\"] = df[\"date\"].dt.normalize()\n",
    "\n",
    "# Convert start and yesterday to naive datetimes for comparison\n",
    "# Convert start and yesterday to naive datetimes for comparison\n",
    "# Normalize to midnight (00:00:00) to ensure exact date matching\n",
    "if start.tz is not None:\n",
    "    start_naive = start.tz_localize(None).normalize()\n",
    "else:\n",
    "    start_naive = start.normalize()\n",
    "\n",
    "if yesterday.tz is not None:\n",
    "    yesterday_naive = yesterday.tz_localize(None).normalize()\n",
    "else:\n",
    "    yesterday_naive = yesterday.normalize()\n",
    "\n",
    "\n",
    "mask = (df[\"local_date\"] >= start_naive) & (df[\"local_date\"] <= yesterday_naive)\n",
    "dfw = df.loc[mask, [\"participant_id\", \"local_date\", \"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\"]].copy()\n",
    "\n",
    "# ========= aggregate to per-participant per-day =========\n",
    "daily = (\n",
    "    dfw.groupby([\"participant_id\", \"local_date\"], as_index=False)[\n",
    "        [\"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\"]\n",
    "    ].sum()\n",
    "    .rename(columns={\"local_date\": \"date\"})\n",
    ")\n",
    "\n",
    "# (ok to keep; will be recomputed later anyway)\n",
    "daily[\"has_accel\"] = daily[\"beiwe_accelerometer_bytes\"] > 0\n",
    "daily[\"has_gps\"]   = daily[\"beiwe_gps_bytes\"] > 0\n",
    "daily[\"has_both\"]  = daily[\"has_accel\"] & daily[\"has_gps\"]\n",
    "daily[\"missing_any\"] = ~daily[\"has_both\"]\n",
    "\n",
    "# ========= ensure full date grid per participant =========\n",
    "# Use naive datetimes for the date range (already computed above)\n",
    "# Normalize all dates to ensure they are at midnight (00:00:00) for proper matching\n",
    "all_days = pd.date_range(start=start_naive, end=yesterday_naive, freq=\"D\").normalize()\n",
    "all_participants = df[\"participant_id\"].dropna().drop_duplicates().sort_values()\n",
    "grid = pd.MultiIndex.from_product([all_participants, all_days], names=[\"participant_id\", \"date\"]).to_frame(index=False)\n",
    "\n",
    "daily_full = grid.merge(daily, on=[\"participant_id\", \"date\"], how=\"left\")\n",
    "\n",
    "# fill bytes first\n",
    "daily_full[[\"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\"]] = (\n",
    "    daily_full[[\"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\"]].fillna(0)\n",
    ")\n",
    "\n",
    "# ========= recompute booleans after fillna =========\n",
    "daily_full[\"has_accel\"] = daily_full[\"beiwe_accelerometer_bytes\"] > 0\n",
    "daily_full[\"has_gps\"]   = daily_full[\"beiwe_gps_bytes\"] > 0\n",
    "daily_full[\"has_both\"]  = daily_full[\"has_accel\"] & daily_full[\"has_gps\"]\n",
    "daily_full[\"missing_any\"] = ~daily_full[\"has_both\"]\n",
    "# ================================================\n",
    "\n",
    "daily_full = daily_full.sort_values([\"participant_id\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "# ========= save =========\n",
    "out_path = f\"daily_sensor_collection_last_{x}_days.csv\"\n",
    "daily_full.to_csv(out_path, index=False)\n",
    "\n",
    "print(\n",
    "    f\"Saved day-level collection matrix for last {x} days to: {out_path}\\n\"\n",
    "    f\"Columns: participant_id, date, beiwe_accelerometer_bytes, beiwe_gps_bytes, has_accel, has_gps, has_both, missing_any\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b89657dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DEBUG: Data for participant 15l9hyfu ===\n",
      "\n",
      "Total rows in data_volume.csv for 15l9hyfu: 760\n",
      "\n",
      "Last 10 days of data:\n",
      "           date  beiwe_accelerometer_bytes  beiwe_gps_bytes timezone                  study_id\n",
      "78   2025-11-03                 14192962.0          37841.0      EST  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "224  2025-11-02                 21589688.0         170500.0      EST  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "318  2025-11-01                 35481957.0         195529.0      EDT  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "476  2025-10-31                 38910140.0         153011.0      EDT  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "640  2025-10-30                 29072095.0         150739.0      EDT  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "801  2025-10-29                  3271561.0          77761.0      EDT  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "913  2025-10-28                 20953559.0          84376.0      EDT  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "1065 2025-10-27                 25954154.0          88637.0      EDT  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "1212 2025-10-26                 22667338.0         307948.0      EDT  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "1319 2025-10-25                 29320007.0         220213.0      EDT  m4z54N5SU7Eqq2LbwmxQd2UN\n",
      "\n",
      "=== Checking last 7 days (from 2025-10-27 to 2025-11-02) ===\n",
      "\n",
      "Rows in last 7 days: 6\n",
      "\n",
      "Data in last 7 days:\n",
      "    local_date  beiwe_accelerometer_bytes  beiwe_gps_bytes timezone\n",
      "224 2025-11-02                 21589688.0         170500.0      EST\n",
      "318 2025-11-01                 35481957.0         195529.0      EDT\n",
      "476 2025-10-31                 38910140.0         153011.0      EDT\n",
      "640 2025-10-30                 29072095.0         150739.0      EDT\n",
      "801 2025-10-29                  3271561.0          77761.0      EDT\n",
      "913 2025-10-28                 20953559.0          84376.0      EDT\n",
      "\n",
      "Aggregated by date (last 7 days):\n",
      "  local_date  beiwe_accelerometer_bytes  beiwe_gps_bytes\n",
      "0 2025-10-28                 20953559.0          84376.0\n",
      "1 2025-10-29                  3271561.0          77761.0\n",
      "2 2025-10-30                 29072095.0         150739.0\n",
      "3 2025-10-31                 38910140.0         153011.0\n",
      "4 2025-11-01                 35481957.0         195529.0\n",
      "5 2025-11-02                 21589688.0         170500.0\n",
      "\n",
      "=== Checking daily_sensor_collection_last_7_days.csv ===\n",
      "\n",
      "Rows for 15l9hyfu in daily_sensor_collection_last_7_days.csv: 7\n",
      "\n",
      "Data in daily_sensor_collection_last_7_days.csv:\n",
      "          date  beiwe_accelerometer_bytes  beiwe_gps_bytes  has_accel  has_gps  has_both\n",
      "7   2025-10-27                 25954154.0          88637.0       True     True      True\n",
      "8   2025-10-28                 20953559.0          84376.0       True     True      True\n",
      "9   2025-10-29                  3271561.0          77761.0       True     True      True\n",
      "10  2025-10-30                 29072095.0         150739.0       True     True      True\n",
      "11  2025-10-31                 38910140.0         153011.0       True     True      True\n",
      "12  2025-11-01                 35481957.0         195529.0       True     True      True\n",
      "13  2025-11-02                 21589688.0         170500.0       True     True      True\n"
     ]
    }
   ],
   "source": [
    "# ========= DEBUG: Check data for participant 15l9hyfu =========\n",
    "import pandas as pd\n",
    "\n",
    "# Load raw data\n",
    "debug_df = pd.read_csv(\"data_volume.csv\")\n",
    "debug_df[\"date\"] = pd.to_datetime(debug_df[\"date\"], errors=\"coerce\")\n",
    "debug_df = debug_df.dropna(subset=[\"date\"])\n",
    "\n",
    "# Filter for participant 15l9hyfu\n",
    "participant_id_debug = \"15l9hyfu\"\n",
    "debug_pid_data = debug_df[debug_df[\"participant_id\"] == participant_id_debug].copy()\n",
    "\n",
    "print(f\"=== DEBUG: Data for participant {participant_id_debug} ===\\n\")\n",
    "print(f\"Total rows in data_volume.csv for {participant_id_debug}: {len(debug_pid_data)}\")\n",
    "\n",
    "if len(debug_pid_data) > 0:\n",
    "    # Show last 10 days of data\n",
    "    debug_pid_data_sorted = debug_pid_data.sort_values(\"date\", ascending=False).head(10)\n",
    "    print(f\"\\nLast 10 days of data:\")\n",
    "    print(debug_pid_data_sorted[[\"date\", \"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\", \"timezone\", \"study_id\"]].to_string())\n",
    "    \n",
    "    # Check last 7 days specifically\n",
    "    from datetime import datetime, timedelta\n",
    "    tz = \"America/New_York\"\n",
    "    yesterday = pd.Timestamp.now(tz=tz).normalize() - pd.Timedelta(days=1)\n",
    "    start = yesterday - pd.Timedelta(days=6)  # last 7 days (inclusive)\n",
    "    \n",
    "    print(f\"\\n=== Checking last 7 days (from {start.strftime('%Y-%m-%d')} to {yesterday.strftime('%Y-%m-%d')}) ===\")\n",
    "    \n",
    "    # Normalize dates for comparison\n",
    "    if debug_pid_data[\"date\"].dt.tz is not None:\n",
    "        debug_pid_data[\"local_date\"] = debug_pid_data[\"date\"].dt.tz_localize(None).dt.normalize()\n",
    "    else:\n",
    "        debug_pid_data[\"local_date\"] = debug_pid_data[\"date\"].dt.normalize()\n",
    "    \n",
    "    start_naive = start.tz_localize(None) if start.tz is not None else start\n",
    "    yesterday_naive = yesterday.tz_localize(None) if yesterday.tz is not None else yesterday\n",
    "    \n",
    "    debug_last7 = debug_pid_data[\n",
    "        (debug_pid_data[\"local_date\"] >= start_naive) & \n",
    "        (debug_pid_data[\"local_date\"] <= yesterday_naive)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nRows in last 7 days: {len(debug_last7)}\")\n",
    "    if len(debug_last7) > 0:\n",
    "        print(\"\\nData in last 7 days:\")\n",
    "        print(debug_last7[[\"local_date\", \"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\", \"timezone\"]].to_string())\n",
    "        \n",
    "        # Aggregate by date\n",
    "        daily_debug = debug_last7.groupby(\"local_date\", as_index=False).agg({\n",
    "            \"beiwe_accelerometer_bytes\": \"sum\",\n",
    "            \"beiwe_gps_bytes\": \"sum\"\n",
    "        })\n",
    "        print(f\"\\nAggregated by date (last 7 days):\")\n",
    "        print(daily_debug.to_string())\n",
    "        \n",
    "        # Check what the daily_full dataframe has\n",
    "        print(f\"\\n=== Checking daily_sensor_collection_last_7_days.csv ===\")\n",
    "        try:\n",
    "            daily_full_check = pd.read_csv(\"daily_sensor_collection_last_7_days.csv\")\n",
    "            daily_full_pid = daily_full_check[daily_full_check[\"participant_id\"] == participant_id_debug].copy()\n",
    "            print(f\"\\nRows for {participant_id_debug} in daily_sensor_collection_last_7_days.csv: {len(daily_full_pid)}\")\n",
    "            if len(daily_full_pid) > 0:\n",
    "                print(\"\\nData in daily_sensor_collection_last_7_days.csv:\")\n",
    "                print(daily_full_pid[[\"date\", \"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\", \"has_accel\", \"has_gps\", \"has_both\"]].to_string())\n",
    "            else:\n",
    "                print(f\"\\n⚠️ NO DATA found for {participant_id_debug} in daily_sensor_collection_last_7_days.csv!\")\n",
    "        except FileNotFoundError:\n",
    "            print(\"\\n⚠️ daily_sensor_collection_last_7_days.csv not found. Run the previous cell first.\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️ NO DATA found for {participant_id_debug} in last 7 days!\")\n",
    "        print(f\"\\nAll available dates for this participant:\")\n",
    "        all_dates = debug_pid_data.sort_values(\"date\")[\"local_date\"].dt.strftime(\"%Y-%m-%d\").unique()\n",
    "        print(f\"Dates available: {', '.join(all_dates[:20])}\")\n",
    "        if len(all_dates) > 20:\n",
    "            print(f\"... and {len(all_dates) - 20} more dates\")\n",
    "else:\n",
    "    print(f\"\\n⚠️ NO DATA found for participant {participant_id_debug} in data_volume.csv at all!\")\n",
    "    print(f\"\\nAvailable participants in data_volume.csv:\")\n",
    "    all_participants = debug_df[\"participant_id\"].unique()\n",
    "    print(f\"Total participants: {len(all_participants)}\")\n",
    "    similar = [p for p in all_participants if \"15l9\" in str(p).lower() or \"l9hy\" in str(p).lower()]\n",
    "    if similar:\n",
    "        print(f\"\\nSimilar participant IDs: {similar[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "671251b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: Filtered data for 15l9hyfu ===\n",
      "Rows after filtering: 6\n",
      "\n",
      "Filtered data:\n",
      "    participant_id local_date  beiwe_accelerometer_bytes  beiwe_gps_bytes\n",
      "224       15l9hyfu 2025-11-02                 21589688.0         170500.0\n",
      "318       15l9hyfu 2025-11-01                 35481957.0         195529.0\n",
      "476       15l9hyfu 2025-10-31                 38910140.0         153011.0\n",
      "640       15l9hyfu 2025-10-30                 29072095.0         150739.0\n",
      "801       15l9hyfu 2025-10-29                  3271561.0          77761.0\n",
      "913       15l9hyfu 2025-10-28                 20953559.0          84376.0\n",
      "\n",
      "Date types:\n",
      "local_date dtype: datetime64[ns]\n",
      "local_date sample: 2025-11-02 00:00:00\n",
      "local_date type: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "\n",
      "=== Step 2: Aggregated data for 15l9hyfu ===\n",
      "Rows in daily: 6\n",
      "\n",
      "Aggregated data:\n",
      "  participant_id       date  beiwe_accelerometer_bytes  beiwe_gps_bytes\n",
      "0       15l9hyfu 2025-10-28                 20953559.0          84376.0\n",
      "1       15l9hyfu 2025-10-29                  3271561.0          77761.0\n",
      "2       15l9hyfu 2025-10-30                 29072095.0         150739.0\n",
      "3       15l9hyfu 2025-10-31                 38910140.0         153011.0\n",
      "4       15l9hyfu 2025-11-01                 35481957.0         195529.0\n",
      "5       15l9hyfu 2025-11-02                 21589688.0         170500.0\n",
      "\n",
      "Date types in daily:\n",
      "date dtype: datetime64[ns]\n",
      "date sample: 2025-10-28 00:00:00\n",
      "date type: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "\n",
      "=== Step 3: Grid data for 15l9hyfu ===\n",
      "Rows in grid: 7\n",
      "\n",
      "Grid data:\n",
      "   participant_id                date\n",
      "7        15l9hyfu 2025-10-27 01:00:00\n",
      "8        15l9hyfu 2025-10-28 01:00:00\n",
      "9        15l9hyfu 2025-10-29 01:00:00\n",
      "10       15l9hyfu 2025-10-30 01:00:00\n",
      "11       15l9hyfu 2025-10-31 01:00:00\n",
      "12       15l9hyfu 2025-11-01 01:00:00\n",
      "13       15l9hyfu 2025-11-02 01:00:00\n",
      "\n",
      "Date types in grid:\n",
      "date dtype: datetime64[ns]\n",
      "date sample: 2025-10-27 01:00:00\n",
      "date type: <class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "\n",
      "=== Step 4: Merge attempt ===\n",
      "\n",
      "Date comparison:\n",
      "  ✗ 2025-10-28 00:00:00 NOT found in grid!\n",
      "    Available grid dates: [Timestamp('2025-10-27 01:00:00'), Timestamp('2025-10-28 01:00:00'), Timestamp('2025-10-29 01:00:00'), Timestamp('2025-10-30 01:00:00'), Timestamp('2025-10-31 01:00:00'), Timestamp('2025-11-01 01:00:00'), Timestamp('2025-11-02 01:00:00')]\n",
      "  ✗ 2025-10-29 00:00:00 NOT found in grid!\n",
      "    Available grid dates: [Timestamp('2025-10-27 01:00:00'), Timestamp('2025-10-28 01:00:00'), Timestamp('2025-10-29 01:00:00'), Timestamp('2025-10-30 01:00:00'), Timestamp('2025-10-31 01:00:00'), Timestamp('2025-11-01 01:00:00'), Timestamp('2025-11-02 01:00:00')]\n",
      "  ✗ 2025-10-30 00:00:00 NOT found in grid!\n",
      "    Available grid dates: [Timestamp('2025-10-27 01:00:00'), Timestamp('2025-10-28 01:00:00'), Timestamp('2025-10-29 01:00:00'), Timestamp('2025-10-30 01:00:00'), Timestamp('2025-10-31 01:00:00'), Timestamp('2025-11-01 01:00:00'), Timestamp('2025-11-02 01:00:00')]\n",
      "  ✗ 2025-10-31 00:00:00 NOT found in grid!\n",
      "    Available grid dates: [Timestamp('2025-10-27 01:00:00'), Timestamp('2025-10-28 01:00:00'), Timestamp('2025-10-29 01:00:00'), Timestamp('2025-10-30 01:00:00'), Timestamp('2025-10-31 01:00:00'), Timestamp('2025-11-01 01:00:00'), Timestamp('2025-11-02 01:00:00')]\n",
      "  ✗ 2025-11-01 00:00:00 NOT found in grid!\n",
      "    Available grid dates: [Timestamp('2025-10-27 01:00:00'), Timestamp('2025-10-28 01:00:00'), Timestamp('2025-10-29 01:00:00'), Timestamp('2025-10-30 01:00:00'), Timestamp('2025-10-31 01:00:00'), Timestamp('2025-11-01 01:00:00'), Timestamp('2025-11-02 01:00:00')]\n",
      "  ✗ 2025-11-02 00:00:00 NOT found in grid!\n",
      "    Available grid dates: [Timestamp('2025-10-27 01:00:00'), Timestamp('2025-10-28 01:00:00'), Timestamp('2025-10-29 01:00:00'), Timestamp('2025-10-30 01:00:00'), Timestamp('2025-10-31 01:00:00'), Timestamp('2025-11-01 01:00:00'), Timestamp('2025-11-02 01:00:00')]\n",
      "\n",
      "Merge result:\n",
      "Rows after merge: 7\n",
      "Merge indicator value_counts:\n",
      "_merge\n",
      "left_only     7\n",
      "right_only    0\n",
      "both          0\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Merged data:\n",
      "  participant_id                date  beiwe_accelerometer_bytes  \\\n",
      "0       15l9hyfu 2025-10-27 01:00:00                        NaN   \n",
      "1       15l9hyfu 2025-10-28 01:00:00                        NaN   \n",
      "2       15l9hyfu 2025-10-29 01:00:00                        NaN   \n",
      "3       15l9hyfu 2025-10-30 01:00:00                        NaN   \n",
      "4       15l9hyfu 2025-10-31 01:00:00                        NaN   \n",
      "5       15l9hyfu 2025-11-01 01:00:00                        NaN   \n",
      "6       15l9hyfu 2025-11-02 01:00:00                        NaN   \n",
      "\n",
      "   beiwe_gps_bytes     _merge  \n",
      "0              NaN  left_only  \n",
      "1              NaN  left_only  \n",
      "2              NaN  left_only  \n",
      "3              NaN  left_only  \n",
      "4              NaN  left_only  \n",
      "5              NaN  left_only  \n",
      "6              NaN  left_only  \n"
     ]
    }
   ],
   "source": [
    "# ========= DEBUG: Check merge issue for participant 15l9hyfu =========\n",
    "# Re-run the processing logic step by step to see where the merge fails\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Replicate the exact logic from the main cell\n",
    "CSV_PATH = \"data_volume.csv\"\n",
    "tz = \"America/New_York\"\n",
    "x = 7  # lookback window\n",
    "study_id = \"m4z54N5SU7Eqq2LbwmxQd2UN\"\n",
    "\n",
    "# Load data\n",
    "df_debug = pd.read_csv(CSV_PATH)\n",
    "df_debug[\"date\"] = pd.to_datetime(df_debug[\"date\"], errors=\"coerce\")\n",
    "df_debug = df_debug.dropna(subset=[\"date\"])\n",
    "df_debug[\"beiwe_accelerometer_bytes\"] = pd.to_numeric(df_debug[\"beiwe_accelerometer_bytes\"], errors=\"coerce\").fillna(0)\n",
    "df_debug[\"beiwe_gps_bytes\"] = pd.to_numeric(df_debug[\"beiwe_gps_bytes\"], errors=\"coerce\").fillna(0)\n",
    "df_debug = df_debug[df_debug[\"study_id\"] == study_id]\n",
    "df_debug = df_debug[df_debug[\"participant_id\"].isin(active_participants)]\n",
    "\n",
    "# Time window\n",
    "yesterday = pd.Timestamp.now(tz=tz).normalize() - pd.Timedelta(days=1)\n",
    "start = yesterday - pd.Timedelta(days=x - 1)\n",
    "\n",
    "# Normalize dates\n",
    "if df_debug[\"date\"].dt.tz is not None:\n",
    "    df_debug[\"local_date\"] = df_debug[\"date\"].dt.tz_localize(None).dt.normalize()\n",
    "else:\n",
    "    df_debug[\"local_date\"] = df_debug[\"date\"].dt.normalize()\n",
    "\n",
    "start_naive = start.tz_localize(None) if start.tz is not None else start\n",
    "yesterday_naive = yesterday.tz_localize(None) if yesterday.tz is not None else yesterday\n",
    "\n",
    "# Filter\n",
    "mask = (df_debug[\"local_date\"] >= start_naive) & (df_debug[\"local_date\"] <= yesterday_naive)\n",
    "dfw_debug = df_debug.loc[mask, [\"participant_id\", \"local_date\", \"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\"]].copy()\n",
    "\n",
    "# Filter for participant 15l9hyfu\n",
    "participant_debug = \"15l9hyfu\"\n",
    "dfw_pid = dfw_debug[dfw_debug[\"participant_id\"] == participant_debug].copy()\n",
    "print(f\"=== Step 1: Filtered data for {participant_debug} ===\")\n",
    "print(f\"Rows after filtering: {len(dfw_pid)}\")\n",
    "if len(dfw_pid) > 0:\n",
    "    print(\"\\nFiltered data:\")\n",
    "    print(dfw_pid[[\"participant_id\", \"local_date\", \"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\"]])\n",
    "    print(f\"\\nDate types:\")\n",
    "    print(f\"local_date dtype: {dfw_pid['local_date'].dtype}\")\n",
    "    print(f\"local_date sample: {dfw_pid['local_date'].iloc[0]}\")\n",
    "    print(f\"local_date type: {type(dfw_pid['local_date'].iloc[0])}\")\n",
    "\n",
    "# Aggregate\n",
    "daily_debug = (\n",
    "    dfw_debug.groupby([\"participant_id\", \"local_date\"], as_index=False)[\n",
    "        [\"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\"]\n",
    "    ].sum()\n",
    "    .rename(columns={\"local_date\": \"date\"})\n",
    ")\n",
    "\n",
    "daily_pid = daily_debug[daily_debug[\"participant_id\"] == participant_debug].copy()\n",
    "print(f\"\\n=== Step 2: Aggregated data for {participant_debug} ===\")\n",
    "print(f\"Rows in daily: {len(daily_pid)}\")\n",
    "if len(daily_pid) > 0:\n",
    "    print(\"\\nAggregated data:\")\n",
    "    print(daily_pid[[\"participant_id\", \"date\", \"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\"]])\n",
    "    print(f\"\\nDate types in daily:\")\n",
    "    print(f\"date dtype: {daily_pid['date'].dtype}\")\n",
    "    print(f\"date sample: {daily_pid['date'].iloc[0]}\")\n",
    "    print(f\"date type: {type(daily_pid['date'].iloc[0])}\")\n",
    "\n",
    "# Create grid\n",
    "all_days_debug = pd.date_range(start=start_naive, end=yesterday_naive, freq=\"D\")\n",
    "all_participants_debug = df_debug[\"participant_id\"].dropna().drop_duplicates().sort_values()\n",
    "grid_debug = pd.MultiIndex.from_product([all_participants_debug, all_days_debug], names=[\"participant_id\", \"date\"]).to_frame(index=False)\n",
    "\n",
    "grid_pid = grid_debug[grid_debug[\"participant_id\"] == participant_debug].copy()\n",
    "print(f\"\\n=== Step 3: Grid data for {participant_debug} ===\")\n",
    "print(f\"Rows in grid: {len(grid_pid)}\")\n",
    "if len(grid_pid) > 0:\n",
    "    print(\"\\nGrid data:\")\n",
    "    print(grid_pid[[\"participant_id\", \"date\"]])\n",
    "    print(f\"\\nDate types in grid:\")\n",
    "    print(f\"date dtype: {grid_pid['date'].dtype}\")\n",
    "    print(f\"date sample: {grid_pid['date'].iloc[0]}\")\n",
    "    print(f\"date type: {type(grid_pid['date'].iloc[0])}\")\n",
    "\n",
    "# Try merge\n",
    "print(f\"\\n=== Step 4: Merge attempt ===\")\n",
    "if len(daily_pid) > 0 and len(grid_pid) > 0:\n",
    "    # Check if dates match\n",
    "    print(\"\\nDate comparison:\")\n",
    "    for idx, row in daily_pid.iterrows():\n",
    "        date_daily = row[\"date\"]\n",
    "        # Check if this date exists in grid\n",
    "        matching = grid_pid[grid_pid[\"date\"] == date_daily]\n",
    "        if len(matching) > 0:\n",
    "            print(f\"  ✓ {date_daily} matches in grid\")\n",
    "        else:\n",
    "            print(f\"  ✗ {date_daily} NOT found in grid!\")\n",
    "            print(f\"    Available grid dates: {grid_pid['date'].tolist()}\")\n",
    "    \n",
    "    # Try the merge\n",
    "    daily_full_debug = grid_pid.merge(daily_pid, on=[\"participant_id\", \"date\"], how=\"left\", indicator=True)\n",
    "    print(f\"\\nMerge result:\")\n",
    "    print(f\"Rows after merge: {len(daily_full_debug)}\")\n",
    "    print(f\"Merge indicator value_counts:\")\n",
    "    print(daily_full_debug[\"_merge\"].value_counts())\n",
    "    \n",
    "    if len(daily_full_debug) > 0:\n",
    "        print(\"\\nMerged data:\")\n",
    "        print(daily_full_debug[[\"participant_id\", \"date\", \"beiwe_accelerometer_bytes\", \"beiwe_gps_bytes\", \"_merge\"]])\n",
    "else:\n",
    "    print(\"Cannot test merge - missing data in daily or grid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "505a6241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active participants with missing data on ALL 7 days: 55\n",
      "(Note: This excludes 'Not Registered' and 'Permanently Retired' participants)\n",
      "(Analysis period: from 2025-10-27 to 2025-11-02)\n",
      "\n",
      "============================================================\n",
      "CATEGORIZED RESULTS (Missing on ALL days):\n",
      "============================================================\n",
      "\n",
      "1. Missing Accel & GPS on ALL 7 days: 51 participants\n",
      "   Participants: ['11hfsajc', '1czziou5', '3284n9os', '32v16fk5', '33vc2l25', '3xhi1mj3', '45mrp24c', '6c5fwr29', '6qmqfkeq', '7kknukw5', '8ucpk1qn', 'ajqv1ibq', 'b4paoqw8', 'bdl5r3es', 'bvavxgux', 'ckfvlrem', 'dpvsqpqf', 'e1kcbop5', 'fk2v9sst', 'i9sd5p7g', 'iadeeya3', 'jei5zxc9', 'jeipst7r', 'khy1h93g', 'l65wznsj', 'mmorx98z', 'mrfmuhvj', 'msj113qe', 'n3soi9u5', 'nudob5v2', 'nwmp8vip', 'o8ga8skc', 'oday9ezi', 'qwpf5ivi', 'rropj9pm', 's14yyl3j', 's63659si', 'sbxh8tts', 'sdnzt337', 'sr2s9fpt', 'swlxsbws', 'teuan5j2', 'thgx9byc', 'txo6lahs', 'v6yrqsb8', 'vyokddrj', 'x6svw2ps', 'xgrb4ude', 'zhcaek8a', 'ztcichn8', 'zwvlgzh4']\n",
      "\n",
      "2. Missing Only GPS on ALL 7 days (has Accel on all days): 4 participants\n",
      "   Participants: ['9ehmj2fz', 'vp49i7uk', 'w57r4125', 'y5vxfnxj']\n",
      "\n",
      "3. Missing Only Accelerometer on ALL 7 days (has GPS on all days): 0 participants\n",
      "\n",
      "============================================================\n",
      "TOTAL ACTIVE PARTICIPANTS WITH DATA ISSUES (missing on ALL 7 days): 55\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "SAVING RESULTS TO CSV FILES:\n",
      "============================================================\n",
      "Saved Missing Accel & GPS on ALL 7 days: output/missing_both_sensors_20251103_122425.csv\n",
      "Saved Missing Only GPS on ALL 7 days (has Accel on all days): output/missing_only_gps_20251103_122425.csv\n",
      "No participants in Missing Only Accelerometer on ALL 7 days (has GPS on all days) category\n",
      "Saved summary file: output/data_quality_summary_20251103_122425.csv\n",
      "\n",
      "============================================================\n",
      "EXPORT COMPLETE - 3 files saved to 'output' folder\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========= CATEGORIZE PARTICIPANTS BY MISSING DATA TYPE (ACTIVE PARTICIPANTS ONLY) =========\n",
    "# Check if data is missing on ALL of the last X days (not just any day)\n",
    "\n",
    "# Group by participant and check if missing on ALL days\n",
    "participant_summary = daily_full.groupby(\"participant_id\").agg({\n",
    "    \"has_accel\": \"all\",  # True only if has_accel is True on ALL days\n",
    "    \"has_gps\": \"all\",    # True only if has_gps is True on ALL days\n",
    "    \"has_both\": \"all\"    # True only if has_both is True on ALL days\n",
    "}).reset_index()\n",
    "\n",
    "# Check missing patterns - missing on ALL days\n",
    "missing_accel_all_days = ~participant_summary[\"has_accel\"]  # True if accel missing on ALL days\n",
    "missing_gps_all_days = ~participant_summary[\"has_gps\"]      # True if GPS missing on ALL days\n",
    "has_accel_all_days = participant_summary[\"has_accel\"]        # True if accel present on ALL days\n",
    "has_gps_all_days = participant_summary[\"has_gps\"]           # True if GPS present on ALL days\n",
    "\n",
    "# Categorize by missing data type (must be missing on ALL X days)\n",
    "missing_accel_gps = []  # Missing both on ALL days\n",
    "missing_only_gps = []   # Has accel on ALL days, missing GPS on ALL days\n",
    "missing_only_accel = [] # Has GPS on ALL days, missing accel on ALL days\n",
    "\n",
    "for idx, row in participant_summary.iterrows():\n",
    "    participant_id = row[\"participant_id\"]\n",
    "    \n",
    "    # Missing both accelerometer and GPS on ALL days\n",
    "    if missing_accel_all_days.iloc[idx] and missing_gps_all_days.iloc[idx]:\n",
    "        missing_accel_gps.append(participant_id)\n",
    "    # Has accelerometer on ALL days but missing GPS on ALL days\n",
    "    elif has_accel_all_days.iloc[idx] and missing_gps_all_days.iloc[idx]:\n",
    "        missing_only_gps.append(participant_id)\n",
    "    # Has GPS on ALL days but missing accelerometer on ALL days\n",
    "    elif has_gps_all_days.iloc[idx] and missing_accel_all_days.iloc[idx]:\n",
    "        missing_only_accel.append(participant_id)\n",
    "\n",
    "# Get total participants with issues (missing something on all days)\n",
    "participants_with_missing = missing_accel_gps + missing_only_gps + missing_only_accel\n",
    "\n",
    "print(f\"Active participants with missing data on ALL {x} days: {len(participants_with_missing)}\")\n",
    "print(f\"(Note: This excludes 'Not Registered' and 'Permanently Retired' participants)\")\n",
    "print(f\"(Analysis period: from {start.strftime('%Y-%m-%d')} to {yesterday.strftime('%Y-%m-%d')})\\n\")\n",
    "\n",
    "# Display categorized results\n",
    "print(\"=\" * 60)\n",
    "print(\"CATEGORIZED RESULTS (Missing on ALL days):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. Missing Accel & GPS on ALL {x} days: {len(missing_accel_gps)} participants\")\n",
    "if missing_accel_gps:\n",
    "    print(f\"   Participants: {missing_accel_gps}\")\n",
    "\n",
    "print(f\"\\n2. Missing Only GPS on ALL {x} days (has Accel on all days): {len(missing_only_gps)} participants\")\n",
    "if missing_only_gps:\n",
    "    print(f\"   Participants: {missing_only_gps}\")\n",
    "\n",
    "print(f\"\\n3. Missing Only Accelerometer on ALL {x} days (has GPS on all days): {len(missing_only_accel)} participants\")\n",
    "if missing_only_accel:\n",
    "    print(f\"   Participants: {missing_only_accel}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"TOTAL ACTIVE PARTICIPANTS WITH DATA ISSUES (missing on ALL {x} days): {len(participants_with_missing)}\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# ========= SAVE RESULTS TO CSV FILES =========\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save each category as a separate CSV file\n",
    "def save_participant_list_to_csv(participant_list, category_name, filename_prefix):\n",
    "    \"\"\"Save a list of participants to CSV with metadata\"\"\"\n",
    "    if participant_list:\n",
    "        df = pd.DataFrame({\n",
    "            'participant_id': participant_list,\n",
    "            'category': category_name,\n",
    "            'count': len(participant_list),\n",
    "            'analysis_date': datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "            'lookback_days': x\n",
    "        })\n",
    "        \n",
    "        filename = f\"{filename_prefix}_{timestamp}.csv\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"Saved {category_name}: {filepath}\")\n",
    "        return filepath\n",
    "    else:\n",
    "        print(f\"No participants in {category_name} category\")\n",
    "        return None\n",
    "\n",
    "# Save categorized results\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING RESULTS TO CSV FILES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "files_saved = []\n",
    "\n",
    "# Save each category\n",
    "files_saved.append(save_participant_list_to_csv(\n",
    "    missing_accel_gps, \n",
    "    f\"Missing Accel & GPS on ALL {x} days\", \n",
    "    \"missing_both_sensors\"\n",
    "))\n",
    "\n",
    "files_saved.append(save_participant_list_to_csv(\n",
    "    missing_only_gps, \n",
    "    f\"Missing Only GPS on ALL {x} days (has Accel on all days)\", \n",
    "    \"missing_only_gps\"\n",
    "))\n",
    "\n",
    "files_saved.append(save_participant_list_to_csv(\n",
    "    missing_only_accel, \n",
    "    f\"Missing Only Accelerometer on ALL {x} days (has GPS on all days)\", \n",
    "    \"missing_only_accelerometer\"\n",
    "))\n",
    "\n",
    "# Save summary file with all categories\n",
    "summary_data = []\n",
    "if missing_accel_gps:\n",
    "    summary_data.extend([{'participant_id': pid, 'category': f'Missing Accel & GPS on ALL {x} days'} for pid in missing_accel_gps])\n",
    "if missing_only_gps:\n",
    "    summary_data.extend([{'participant_id': pid, 'category': f'Missing Only GPS on ALL {x} days (has Accel on all days)'} for pid in missing_only_gps])\n",
    "if missing_only_accel:\n",
    "    summary_data.extend([{'participant_id': pid, 'category': f'Missing Only Accelerometer on ALL {x} days (has GPS on all days)'} for pid in missing_only_accel])\n",
    "\n",
    "if summary_data:\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df['analysis_date'] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    summary_df['lookback_days'] = x\n",
    "    \n",
    "    summary_filename = f\"data_quality_summary_{timestamp}.csv\"\n",
    "    summary_filepath = os.path.join(output_dir, summary_filename)\n",
    "    summary_df.to_csv(summary_filepath, index=False)\n",
    "    print(f\"Saved summary file: {summary_filepath}\")\n",
    "    files_saved.append(summary_filepath)\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"EXPORT COMPLETE - {len([f for f in files_saved if f])} files saved to '{output_dir}' folder\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ForestEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
