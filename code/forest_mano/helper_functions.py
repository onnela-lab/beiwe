from pathlib import Path
from itertools import islice
import numpy as np
import pandas as pd
from pandas import json_normalize
import orjson
import os
import sys
from datetime import datetime
from datetime import timedelta
import pytz
import math
from functools import reduce
from datetime import datetime
import mano
import mano.sync as msync
import requests

space =  '    '
branch = '│   '
tee =    '├── '
last =   '└── '

def tree(dir_path: Path, level: int=-1, limit_to_directories: bool=False,
         length_limit: int=1000):
    """Given a directory Path object print a visual tree structure"""
    dir_path = Path(dir_path) # accept string coerceable to Path
    files = 0
    directories = 0
    def inner(dir_path: Path, prefix: str='', level=-1):
        nonlocal files, directories
        if not level: 
            return # 0, stop iterating
        if limit_to_directories:
            contents = [d for d in dir_path.iterdir() if d.is_dir()]
        else: 
            contents = list(dir_path.iterdir())
        pointers = [tee] * (len(contents) - 1) + [last]
        for pointer, path in zip(pointers, contents):
            if path.is_dir():
                yield prefix + pointer + path.name
                directories += 1
                extension = branch if pointer == tee else space 
                yield from inner(path, prefix=prefix+extension, level=level-1)
            elif not limit_to_directories:
                yield prefix + pointer + path.name
                files += 1
    print(dir_path.name)
    iterator = inner(dir_path, level=level)
    for line in islice(iterator, length_limit):
        print(line)
    if next(iterator, None):
        print(f'... length_limit, {length_limit}, reached, counted:')
    print(f'\n{directories} directories' + (f', {files} files' if files else ''))


def concatenate_summaries(dir_path: Path, output_filename: str):
    """Concatenate subject-specific GPS- or communication-related summaries
    
    Checks to see if there is an hourly or daily folder first, then concatenates sub-folders first. 
    """
    dir_path = Path(dir_path) # accept string coerceable to Path
    if os.path.exists(dir_path / "hourly"):
        concatenate_folder(dir_path / "hourly", output_filename[0:-4] + "_hourly" + ".csv")
    if os.path.exists(dir_path / "daily"):
        concatenate_folder(dir_path / "daily", output_filename[0:-4] + "_daily" + ".csv")
    concatenate_folder(dir_path, output_filename)
        
    
    
  
  
def concatenate_folder(dir_path: Path, output_filename: str):
    """Concatenate one folder of GPS- or communication-related summaries"""
    
    # initialize dataframe list
    df_list = []

    # loop through files in dir_path
    for file in os.listdir(dir_path):        
        # obtain subject study_id 
        file_dir = os.path.join(dir_path,file)
        subject_id = os.path.basename(file_dir)[:-4]
        if file.endswith(".csv"): 
            temp_df = pd.read_csv(file_dir)
            temp_df.insert(loc=0, column='Beiwe_ID', value=subject_id)
            df_list.append(temp_df)
                
    if len(df_list) > 0:
                    
        # concatenate dataframes within list --> Final Data for trajectories
        response_data = pd.concat(df_list, axis=0)

        # make directory 
        os.makedirs(dir_path / "concatenated", exist_ok=True) 
        path_resp = os.path.join(dir_path / "concatenated", output_filename)    

        # write to csv
        response_data.to_csv(path_resp, index=False)
        print("Concatenated folder " + str(dir_path) + " to " + str(output_filename))
    else:
        print("No input data found in folder " + str(dir_path))

# Convert study time to UTC
def convert_to_utc_and_format(date_str, time_str, timezone_str):
    local_time = datetime.strptime(f"{date_str} {time_str}", "%Y-%m-%d %H:%M:%S")
    local_timezone = pytz.timezone(timezone_str)
    local_time = local_timezone.localize(local_time)
    utc_time = local_time.astimezone(pytz.utc)
    utc_time_str = utc_time.strftime("%Y-%m-%dT%H:%M:%S")
    
    return utc_time_str

def download_data(keyring, study_id, download_folder, tz_str, users = [], time_start = "2008-01-01", 
                      time_end = None, data_streams = None):
    '''
    Downloads all data for specified users, time frame, and data streams. 
    
    This function downloads all data for selected users, time frame, and data streams, and writes them to an 
    output folder, with one subfolder for each user, and subfolders inside the user's folder for each data stream. 
    If a server failure happens, the function re-attempts the download. 
    
    Args: 
        keyring: a keyring generated by mano.keyring
    
        users(iterable): A list of users to download data for. If none are entered, it attempts to download data for all users
        
        study_id(str): The id of a study
        
        download_folder(str): path to a folder to download data

        tz_str(str): The study timezone

        time_start(str): The initial date to download data (Formatted in YYYY-MM-DD). Default is 2008-01-01, which is 
            before any Beiwe data existed.
        
        time_end(str): The date to end downloads. The default is today at midnight.

        data_streams(iterable): A list of all data streams to download. The default (None) is all possible data streams. 
        
    '''
    if study_id == "":
        print("Error: Study ID is blank")
        return
        
    if (keyring['USERNAME'] == "" or keyring['PASSWORD'] == "" 
        or keyring["ACCESS_KEY"] == "" or keyring["SECRET_KEY"] == ""):
        print("Error: Did you set up the keyring_studies.py file?")
        return
    
    if not os.path.isdir(download_folder):
        os.mkdir(download_folder)
    
     if tz_str == "":
        print("Error: Timezone is blank")
        return

    local_timezone = pytz.timezone(tz_str)

    if time_end is None:
        time_end = datetime.today().strftime("%Y-%m-%d")+"T23:59:00"
    else:
        time_end = convert_to_utc_and_format(time_end, "23:59:00", tz_str)

    time_start = convert_to_utc_and_format(time_start, "00:00:00", tz_str)
    
    if users == []:
        print('Obtaining list of users...')
        num_tries = 1
        while num_tries < 5:
            try:
                users = [u for u in mano.users(keyring, study_id)]
                num_tries = 6
            except KeyboardInterrupt:
                print("Someone closed the program")
                sys.exit()
            except mano.APIError as e:
                print("Something is wrong with your credentials:")
                print(e)
                num_tries = 6
            except:
                num_tries = num_tries + 1
        
            
    
    for u in users:
        zf = None
        download_success = False
        num_tries = 0
        while not download_success:
            try:
                print(f'Downloading data for {u}')
                zf = msync.download(keyring, study_id, u, data_streams, time_start = time_start, time_end = time_end)
                if zf is not None:
                    zf.extractall(download_folder)
                download_success = True
            except requests.exceptions.ChunkedEncodingError:
                print(f'Network failed in download of {u}, try {num_tries}')
                num_tries = num_tries + 1
            except KeyboardInterrupt:
                print("Someone closed the program")
                sys.exit()
            except mano.APIError as e:
                print("Something is wrong with your credentials:")
                print(e)
                download_success = True
            if num_tries > 5:
                download_success = True
                print(f"Too many failures; skipping user {u}")
        if zf is None:
            print(f'No data for {u}; nothing written')

def call_api(endpoint, study_id, access_key, secret_key):
    '''
    Calls a specific Beiwe API to gather different pieces of information about a study. 
    
    This function calls one of three possible API endpoints: get-summary-statistics/v1, {FINISH} registration info and study settings? and the
    passes along the json response as a pandas dataframe
     
    Args: 
        endpoint: a key for which endpoint should be accessed {}
    
        study_id(str): The id of a study
        
        access_key: API access key from the keyring file

        secret_key: API secret key from the keyring file 
        
    '''
    # make a post request to the get-participant-upload-history/v1 endpoint, including the api key,
    # secret key, and participant_id as post parameters.
    t_start = datetime.now()
    print("Starting request at", t_start, flush=True)
    response = requests.post(
        endpoint,
        
        # refine your parameters here
        data={
            "access_key": access_key,
            "secret_key": secret_key,
            
            # several endpoints take a participant id, 
            "study_id": study_id,
            
            # `omit_keys` is an option on some endpoints, it causes the return data to be potentially
            # much smaller and faster. Format of data will replace dictions with lists of values, order
            # will be retained. It takes a string, "true" or "false".
            # "omit_keys": "true",
            "data_format": "json"
            # etc.
            
        },
        allow_redirects=False,
    )
    t_end = datetime.now()
    print("Request completed at", t_end.isoformat(), "duration:", (t_end - t_start).total_seconds(), "seconds")
    
    status_code = response.status_code
    raw_output = response.content
    
    # the rest is just some sanity checking to make sure the your request worked and give you basic
    # feedback.
    print("http status code:", response.status_code)
    
    assert status_code != 400, \
        "400 usually means you are missing a required parameter, or something critical isn't passing some checks.\n" \
        "Check your access key and secret key, if there is a study id make sure it is 24 characters long."
    
    assert status_code != 403, \
        "Permissions Error, you are not authenticated to view data on this study."
    
    assert status_code != 404, \
        "404 means that the entity you have specified does not exist. Check details like study_id, patient_id, etc."
    
    assert response.status_code != 301, \
        "Encountered HTTP redirect, you may have forgotten the s in https. first 100 bytes of response:\n" \
        f"{raw_output[:100]}"
    
    assert response.content != b"", "No data was returned by the server..."
    
    print("Testing whether it is valid json...")
    try:
        json_response = orjson.loads(response.content)
        print("JSON successfully loadded into variable `json_response`")
    except orjson.JSONDecodeError:
        print("Not valid JSON - which may or may not be an issue! Here is the raw output of the first 100 bytes:")
        print(raw_output[:100])
        json_response = None
    
    return json_normalize(json_response)